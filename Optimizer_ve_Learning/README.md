# Optimizer ve Learning Rate (Ã–ÄŸrenme OranÄ±) Ä°ncelemesi

Bu proje, Derin Ã–ÄŸrenme (Deep Learning) alanÄ±ndaki iki temel kavram olan **Optimizer (EniyileÅŸtirici Algoritmalar)** ve **Learning Rate (Ã–ÄŸrenme OranÄ±)** kavramlarÄ±nÄ±n etkilerini incelemek iÃ§in oluÅŸturulmuÅŸtur. Bu doÄŸrultuda, bir regresyon problemi (Ev Fiyat Tahmini) Ã¼zerinde **SGD (Stochastic Gradient Descent)** ve **Adam** optimizer algoritmalarÄ± arasÄ±ndaki baÅŸarÄ± ve hÄ±z farklÄ±lÄ±ÄŸÄ± simÃ¼le edilmiÅŸtir.

## ğŸ“ Proje Dosya YapÄ±sÄ±

KlasÃ¶rÃ¼nÃ¼zdeki karmaÅŸÄ±klÄ±ÄŸÄ± gidermek adÄ±na dosyalar dÃ¼zenlenmiÅŸ ve daha profesyonel, anlaÅŸÄ±lÄ±r bir formata getirilmiÅŸtir:
- `train.csv`: Model eÄŸitiminde kullanÄ±lan baÄŸÄ±msÄ±z (X) ve baÄŸÄ±mlÄ± (y) deÄŸiÅŸkenleri iÃ§eren veri seti.
- `optimizer_comparison.py` *(eski adÄ±yla `orn.py`)*: SGD ve Adam optimizasyon tekniklerini aynÄ± derin Ã¶ÄŸrenme mimarisi Ã¼zerinde test edip karÅŸÄ±laÅŸtÄ±ran Python betiÄŸi. 
- `optimizer_comparison.png` *(eski adÄ±yla `Ekran gÃ¶rÃ¼ntÃ¼sÃ¼ 2026-02-26 142411.png`)*: Modelin eÄŸitim sÃ¼reci neticesinde iki optimizer'Ä±n "DoÄŸrulama HatasÄ± (Validation Loss)" dÃ¼ÅŸÃ¼ÅŸ hÄ±zÄ±nÄ± gÃ¶steren karÅŸÄ±laÅŸtÄ±rma grafiÄŸi.
- `performance.png`: Modelin farklÄ± eÄŸitim ve test aÅŸamalarÄ±ndaki performans metriklerini ve deÄŸerlendirmelerini iÃ§eren gÃ¶rsel.
- `README.md`: TÃ¼m bu yapÄ±larÄ± ve matematiksel denklemleri aÃ§Ä±klayan dokÃ¼mantasyon (ÅŸu an okuduÄŸunuz doysa).

---

## ğŸ§  Konu BaÅŸlÄ±ÄŸÄ±: Optimizer ve Learning Rate Nedir?

Yapay zeka modellerini eÄŸitirken, modelin yaptÄ±ÄŸÄ± hatalarÄ± en aza indirmek (Loss deÄŸerini minimize etmek) isteriz. Modeli daha baÅŸarÄ±lÄ± kÄ±lmak iÃ§in aÄŸÄ±rlÄ±klarÄ±n (weights) hangi yÃ¶nde ve ne boyutta gÃ¼ncelleneceÄŸini belirleyen yÃ¶ntemlere **Optimizer** (EniyileÅŸtirici) denir. Bu gÃ¼ncellemelerin adÄ±m bÃ¼yÃ¼klÃ¼ÄŸÃ¼nÃ¼ kontrol eden ve modelin eÄŸilimini ayarlayan hiperparametreye ise **Learning Rate (Ã–ÄŸrenme OranÄ±, $\alpha$)** adÄ± verilir.

### 1. Learning Rate (Ã–ÄŸrenme OranÄ±)
Learning Rate, genellikle $\alpha$ (alfa) ile gÃ¶sterilir ve $[0, 1]$ aralÄ±ÄŸÄ±nda (ancak genellikle $0.001$, $0.01$ gibi Ã§ok kÃ¼Ã§Ã¼k deÄŸerler alan) pozitif bir sayÄ±dÄ±r.
- **Ã‡ok BÃ¼yÃ¼k Bir $\alpha$:** Model, optimum noktayÄ± (minimum hatayÄ± bulduÄŸumuz noktayÄ±) atlayabilir. SÃ¼rekli sekerek kayÄ±p (loss) grafiÄŸinde sapmalara sebep olur.
- **Ã‡ok KÃ¼Ã§Ã¼k Bir $\alpha$:** Model Ã§ok yavaÅŸ Ã¶ÄŸrenir. Hatta bazen "local minima" dediÄŸimiz lokal Ã§ukurlara takÄ±lÄ±p kalÄ±r ve optimum seviyeye ulaÅŸmasÄ± Ã§ok uzun (epoch) sÃ¼rer.

**Matematiksel AÄŸÄ±rlÄ±k GÃ¼ncellemesi KuralÄ±:**
FormÃ¼lÃ¼n en temel yapÄ±sÄ± ÅŸÃ¶yledir:
$$ \theta_{yeni} = \theta_{eski} - \alpha \cdot \nabla J(\theta) $$
Burada $\nabla J(\theta)$, hata/kayÄ±p (Loss) fonksiyonunun tÃ¼revidir (Gradyan). Biz aÄŸÄ±rlÄ±klarÄ±mÄ±zÄ± hatanÄ±n tersi yÃ¶nde, gradyanÄ± baz alarak gÃ¼ncelleriz.

---

### 2. SGD (Stochastic Gradient Descent)
Standart Gradient Descent, hesaplama yapmak iÃ§in tÃ¼m veri setini dikkate alÄ±r, bu da devasa veri setlerinde Ã§ok maliyetlidir ve yavaÅŸtÄ±r. SGD ise eÄŸitim verisinden dÃ¼zenli olarak alÄ±nan rastgele Ã¶rneklemler Ã¼zerinden her adÄ±mda hemen parametreleri gÃ¼nceller.
Ã‡ok daha az hesaplama gÃ¼cÃ¼ ister, fakat her bir iterasyondaki gradyan rastgele (stochastic) alÄ±ndÄ±ÄŸÄ±ndan rotasÄ± Ã§ok "gÃ¼rÃ¼ltÃ¼lÃ¼dÃ¼r" (iniÅŸli Ã§Ä±kÄ±ÅŸlÄ±).

**Matematiksel Ä°fadesi:**
Her $\theta$ (aÄŸÄ±rlÄ±k) deÄŸeri, rastgele seÃ§ilen i. Ã¶rnek iÃ§in $x^{(i)}$ girdisi ve $y^{(i)}$ hedefi olmak Ã¼zere ÅŸÃ¶yle gÃ¼ncellenir:
$$ \theta_{t} = \theta_{t-1} - \alpha \cdot \nabla_{\theta} J(\theta_{t-1}; x^{(i)}, y^{(i)}) $$

---

### 3. Adam Optimizer (Adaptive Moment Estimation)
En popÃ¼ler ve gÃ¼nÃ¼mÃ¼zde en Ã§ok varsayÄ±lan olarak kabul edilen optimizer yÃ¶ntemidir. Matematiksel olarak Momentum ve RMSProp optimizasyon algoritmalarÄ±nÄ±n yeteneklerini birleÅŸtirir.
Ã–ÄŸrenme oranÄ±nÄ± (Learning Rate) her bir aÄŸÄ±rlÄ±k parametresi iÃ§in duruma gÃ¶re **bireysel ve dinamik (Adaptive)** olarak ayarlar.

**Matematiksel Ä°fadesi (Ã‡Ã¶zÃ¼mleniÅŸi):**
Adam Optimizer, Ã¶nceki gradyanlarÄ±n ortalama deÄŸerlerini (momentum) ve gradyan karesini akÄ±lda tutarak hesaplama yapar.

**a.** 1. Moment Tahmini (Mean/Ortalama - EÄŸimin ne yÃ¶nde gittiÄŸi):  
$$ m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t $$  
*(Burada $g_t$ gÃ¼ncel gradyan / hata tÃ¼revidir. $\beta_1$ genelde 0.9'dur)*

**b.** 2. Moment Tahmini (Uncentered Variance/Varyans - EÄŸimin ne kadar bÃ¼yÃ¼k/kÃ¼Ã§Ã¼k olduÄŸu):  
$$ v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 $$  
*(Burada $\beta_2$ genelde 0.999'dur)*

**c.** BaÅŸlangÄ±Ã§ DÃ¼zeltmesi (Bias Correction - BaÅŸlangÄ±Ã§taki sÄ±fÄ±ra kayma eÄŸilimini Ã¶nlemek):  
$$ \hat{m}_t = \frac{m_t}{1 - \beta_1^t}  \quad , \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t} $$

**d.** Parametrelerin GÃ¼ncellenmesi KuralÄ±:  
$$ \theta_t = \theta_{t-1} - \alpha \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} $$  
*Burada $\epsilon$ (epsilon) paydanÄ±n sÄ±fÄ±r olmasÄ±nÄ± engellemek iÃ§in eklenen Ã§ok kÃ¼Ã§Ã¼k bir sabit (~$10^{-8}$).*

GÃ¶rÃ¼ldÃ¼ÄŸÃ¼ Ã¼zere Adam, gradyan deÄŸiÅŸiminin yoÄŸun olduÄŸu, karmaÅŸÄ±k dÃ¼zlemlerde hÄ±zÄ± dinamik olarak kÄ±sÄ±p/aÃ§arak Ã§ok daha hÄ±zlÄ± ve pÃ¼rÃ¼zsÃ¼z bir yakÄ±nlaÅŸma (convergence) saÄŸlar.

---

## ğŸ“ˆ SonuÃ§larÄ±n GÃ¶rselleÅŸtirilmesi ve Analizi

![Adam vs SGD KarÅŸÄ±laÅŸtÄ±rma GrafiÄŸi](optimizer_comparison.png)

Projedeki `optimizer_comparison.py` betiÄŸi ile MSE (Hata OranÄ±) logaritmik olarak aÅŸaÄŸÄ±daki farklÄ± sonuÃ§larla incelenmiÅŸtir:
* **Mavi Ã‡izgi (Adam):** Adam, hata oranÄ±nÄ± (Val Loss) ilk epoch'lardan itibaren **Ã§ok agresif ve hÄ±zlÄ±** bir ÅŸekilde dÃ¼ÅŸÃ¼rmÃ¼ÅŸtÃ¼r. Momentum ve RMSProp mimarisinin getirdiÄŸi dinamik Ã¶ÄŸrenme yeteneÄŸi sayesinde minimum hataya kolayca ulaÅŸÄ±r. 
* **KÄ±rmÄ±zÄ± Ã‡izgi (SGD):** SGD, mavi Ã§izgiye kÄ±yasla daha Ã§ok sabit/yatay bir eÄŸilim gÃ¶stermektedir veya daha nazlÄ± bir hÄ±zla dÃ¼ÅŸmektedir. Ã‡Ã¶zÃ¼m uzayÄ±nda bir yÃ¶ne kararlÄ± gitmez ve yavaÅŸlÄ±ÄŸÄ± onu geride sÄ±klÄ±kla bÄ±rakÄ±r. Sabit veriler Ã¼zerinde parametrelerini stabilize etmesi daha Ã§ok epoch sayÄ±sÄ±na gereksinim duyar.

`optimizer_comparison.py` Ã§alÄ±ÅŸtÄ±rÄ±larak hata dÃ¼ÅŸÃ¼ÅŸ loglarÄ±na ve bu grafiÄŸin gÃ¼ncel haline ulaÅŸabilirsiniz.

### ğŸ“Š Performans DeÄŸerlendirmesi

AlgoritmalarÄ±n hata (Loss) azaltma hÄ±zlarÄ±nÄ±n yanÄ± sÄ±ra genel performanslarÄ±nÄ±n analizi aÅŸaÄŸÄ±da yer almaktadÄ±r:

![Performans KÄ±yaslamasÄ±](performance.png)

Bu gÃ¶rsel, modellerin tahmin yeteneklerinin ve genel hata/baÅŸarÄ± metriklerinin birbirlerine kÄ±yasla nasÄ±l ÅŸekillendiÄŸini desteklemek amacÄ±yla verilmiÅŸtir.
